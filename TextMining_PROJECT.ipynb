{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextMining-PROJECT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvaneshkj/Natural-Language-Processing/blob/master/TextMining_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr_3aDveKU1g"
      },
      "source": [
        "#TEXT SUMMARIZATION (BERTSUM, GPT2, XLnet, Cosine Similarity)\n",
        "#Text Mining Project\n",
        "#Bhuvanesh Jeevarathinam, Sushmitha Mani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy3vRmWJWAe0"
      },
      "source": [
        "Installation of necessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL_CUUXgLU2a"
      },
      "source": [
        "!pip install bert-extractive-summarizer==0.4.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO4KjEH3LU41"
      },
      "source": [
        "!pip install transformers==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA8rMAB3LU7W"
      },
      "source": [
        "!pip install spacy==2.0.12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06kc4A6rLU95"
      },
      "source": [
        "!pip install rouge-score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39rIN2garjMC"
      },
      "source": [
        "!pip install flask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8pKaylwLVAY"
      },
      "source": [
        "!pip install git+https://github.com/tagucci/pythonrouge.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86CWnGCNLVCy"
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLw5NI26LVGV"
      },
      "source": [
        "!pip install gtts\n",
        "!pip install SpeechRecognition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b8t9zxOXSP8"
      },
      "source": [
        "## Speech To Text using Google GTTS Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yz5FpPGLVIv"
      },
      "source": [
        "def bot_speak(voice_data):\n",
        "\n",
        "    if \"how are you\" in voice_data.lower():\n",
        "        audio_string = \"I'm great and you?\"\n",
        "    elif \"how old are you\" in voice_data.lower():\n",
        "        audio_string = \"I never give my age.\"\n",
        "    elif 'bitcoin' in voice_data.lower():\n",
        "        r = requests.get('https://api.coingecko.com/api/v3/simple/price?ids=bitcoin&vs_currencies=usd')\n",
        "        a = json.loads(r.text)\n",
        "        audio_string = \"1 bitcoin will cost you \" + str(a[\"bitcoin\"][\"usd\"]) + \" US dollars.\"\n",
        "    elif 'launch youtube' in voice_data.lower():\n",
        "        url = \"https://www.youtube.com/\"\n",
        "        webbrowser.get().open(url)\n",
        "        audio_string = \"Opening YouTube\"\n",
        "    elif 'search youtube' in voice_data.lower():\n",
        "        keyword = voice_data.split(\"for\")[-1]\n",
        "\n",
        "        url = f\"https://www.youtube.com/results?search_query={keyword}\"\n",
        "        webbrowser.get().open(url)\n",
        "        audio_string = \"Search YouTube for \" + str(keyword)\n",
        "    else:\n",
        "        audio_string = \"I'd didn't get that sorry.\"\n",
        "    \n",
        "    tts = gTTS(text=audio_string, lang=\"en\")\n",
        "    audio_file = 'C://Users/chat2/Jupyter_code/Text Mining/AudioFiles/audio.mp3'\n",
        "    tts.save(audio_file)\n",
        "    playsound.playsound(audio_file)\n",
        "    os.remove(audio_file)\n",
        "\n",
        "#bot_speak()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PM9EG3jXtiU"
      },
      "source": [
        "# Youtube Video Text Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeqxqRCrXvdp"
      },
      "source": [
        "!pip install youtube_dl\n",
        "!pip install youtube-transcript-api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0tukLnoqujw"
      },
      "source": [
        "# Sample Scrum Call https://www.youtube.com/watch?v=nTtru5CfwbM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyQZi2FKXl7O"
      },
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "YT_Text=YouTubeTranscriptApi.get_transcript('nTtru5CfwbM')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiDuLsEVYyic"
      },
      "source": [
        "List_Text=[]\n",
        "for i in range(0, len(YT_Text)):\n",
        "        print(YT_Text[i]['text'])\n",
        "        List_Text.append(YT_Text[i]['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c67MeYG8Y71n"
      },
      "source": [
        "Processed_Text = ' '.join(List_Text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl7y30vnY74x"
      },
      "source": [
        "Processed_Text=Processed_Text.replace('\\n',' ')\n",
        "Processed_Text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvEZ1OKMZJL_"
      },
      "source": [
        "#Adding Punctuator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKmTZZL3ZYOU"
      },
      "source": [
        "!pip install punctuator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuMpLkXTaP9Q"
      },
      "source": [
        "Processed_Text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emfjrmYmaQQG"
      },
      "source": [
        "##  Should Download and Refer the Pretrained Model from https://drive.google.com/drive/folders/0B7BsN5f2F1fZQnFsbzJ3TWxxMms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7APyQ3uZYL5"
      },
      "source": [
        "from punctuator import Punctuator\n",
        "punctuate_model = Punctuator('/content/sample_data/INTERSPEECH-T-BRNN.pcl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej7V8MsfZcgb"
      },
      "source": [
        "punctuated_text=punctuate_model.punctuate(Processed_Text)\n",
        "print(punctuated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mHuSkfJkOOu"
      },
      "source": [
        "# **Reading the data:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKhxBAd1Y4OA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZfUrfKhL6uM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from summarizer import Summarizer\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NQ65EuRbK8G"
      },
      "source": [
        "***Can Also Upload a Text file Directly.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1GIhNhtL6wv"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt6fKidKL6zv"
      },
      "source": [
        "with open(\"/content/textmining.txt\") as f:\n",
        "  text_data = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFEYZ5JWL62v"
      },
      "source": [
        "text_data=punctuated_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5T1To8uL65g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rPpDcz1L6-0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vOH2RaVTHqQ"
      },
      "source": [
        "**BERTSUM**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0X2v2DuL7BZ"
      },
      "source": [
        "from summarizer import Summarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8EB13gYL7E3"
      },
      "source": [
        "def Bert_Sum(input_data): \n",
        "  model = Summarizer()\n",
        "  result = model(input_data, min_length=150, max_length=512)\n",
        "  summary_bert = \"\".join(result)\n",
        "  print(summary_bert)\n",
        "  return summary_bert\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_23sN6z5L7IL"
      },
      "source": [
        "model1 = Bert_Sum(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7e3SWJ5L7MT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuAK0KryS6-m"
      },
      "source": [
        "**GPT2**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVr8zc7VrdC"
      },
      "source": [
        "from summarizer import TransformerSummarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0iU7GQbL7RI"
      },
      "source": [
        "#GPT2 \n",
        "\n",
        "def GPT2_sum(input_data):\n",
        "  GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
        "  summary_gpt2 = ''.join(GPT2_model(input_data, min_length=250, max_length=1024))\n",
        "  print('The Summary of the GPT2 model is as follows:  \\n')\n",
        "  print(summary_gpt2)\n",
        "  return summary_gpt2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A118kBG2L68M"
      },
      "source": [
        "model2= GPT2_sum(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhlT2SCHNv7N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCZzllWyN2S_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6RKPAQp6pcv"
      },
      "source": [
        "**XLNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IkVCry0N3uD"
      },
      "source": [
        "from summarizer import TransformerSummarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHnzFIZHN36g"
      },
      "source": [
        "#XLNet\n",
        "\n",
        "def XLNet_sum(input_data):\n",
        "  model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-base-cased\")\n",
        "  summary_xlnet = ''.join(model(input_data, min_length=20, max_length=400))\n",
        "  print(summary_xlnet)\n",
        "  return summary_xlnet\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEvLy_StN38-"
      },
      "source": [
        "model3= XLNet_sum(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSb65ike501q"
      },
      "source": [
        "model3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrnB713G6TV4"
      },
      "source": [
        "!pip install playsound"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSAZ5ko_OraN"
      },
      "source": [
        "from gtts import gTTS\n",
        "import playsound\n",
        "from IPython.display import Audio\n",
        "import os\n",
        "def bot_speak(voice_data): \n",
        "    tts = gTTS(text=voice_data, lang=\"en\")\n",
        "    audio_file = 'audio.mp3'\n",
        "    tts.save(audio_file)\n",
        "    Audio(audio_file, autoplay=True) \n",
        "    #playsound.playsound(audio_file)\n",
        "    os.remove(audio_file)\n",
        "\n",
        "bot_speak(model3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIQOi7-HOrUC"
      },
      "source": [
        "Audio('/content/audio.mp3', autoplay=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV12c2r6N4B6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMz6KBAcmoM1"
      },
      "source": [
        "**ROUGE SCORE- BERT**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R2NoGRrkTOX"
      },
      "source": [
        "#reading the human generated summary for the text data\n",
        "with open(\"/content/golden_summary.txt\") as f:\n",
        "  golden = f.read()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6U7nNUEk4ei"
      },
      "source": [
        "golden ='Text mining is used for specific research questions like why cats sit on mats. It is not just a search tool but it filters and extracts relevant information. For example, this program would identify the cat is in action and man is the object and it maps the trend across the articles. It helps to determine the additional research needed to find out questions and answers.  '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgUMdqZcverW"
      },
      "source": [
        "#BLEU Score- Bilingual Evaluation Understudy \n",
        "#from nltk.translate.bleu_score import sentence_bleu\n",
        "#reference = text_data\n",
        "#candidate = summary\n",
        "#score = sentence_bleu(reference, candidate)\n",
        "#print('The BLEU score of the BERT Model is:', score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKamxFGyM7SK"
      },
      "source": [
        "#Rouge Score Comparison of the machine Summary with the original text data\n",
        "reference = golden #Refers to the human summary\n",
        "candidate = model1 #refers to machine summary\n",
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores_bert = scorer.score(reference, candidate)\n",
        "print(scores_bert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPrPzRSJN4EQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_UuLepxM7VH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T24TgkGcm1km"
      },
      "source": [
        "**ROUGE SCORE - GPT2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUlfxZXLM7Pd"
      },
      "source": [
        "#Rouge Score Comparison of the machine Summary with the original text data\n",
        "from rouge_score import rouge_scorer\n",
        "reference = golden \n",
        "candidate = model2\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores_gpt2 = scorer.score(reference, candidate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJcFo6TJnXPJ"
      },
      "source": [
        "print(scores_gpt2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8kFsSYinP5g"
      },
      "source": [
        "**ROUGE SCORE - XLNET**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdPVML0zM7KL"
      },
      "source": [
        "#Rouge Score Comparison of the machine Summary with the original text data\n",
        "from rouge_score import rouge_scorer\n",
        "reference = golden \n",
        "candidate = model3\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores_xlnet = scorer.score(reference, candidate)\n",
        "print(scores_xlnet)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QGNPocfr_j9"
      },
      "source": [
        "# Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J0iyPXana3f"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from heapq import nlargest\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIucKq0fN4Gm"
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA3PdjDCPBOd"
      },
      "source": [
        "doc =nlp(text_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjs0W6OQPBRj"
      },
      "source": [
        "len(list(doc.sents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUmyKLb5PBV7"
      },
      "source": [
        "keyword = []\n",
        "stopwords = list(STOP_WORDS)\n",
        "pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
        "for token in doc:\n",
        "    if(token.text in stopwords or token.text in punctuation):\n",
        "        continue\n",
        "    if(token.pos_ in pos_tag):\n",
        "        keyword.append(token.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpvDnJWqe_Lr"
      },
      "source": [
        "freq_word = Counter(keyword)\n",
        "print(freq_word.most_common(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0clb0vPKe_JH"
      },
      "source": [
        "max_freq = Counter(keyword).most_common(1)[0][1]\n",
        "for word in freq_word.keys():  \n",
        "        freq_word[word] = (freq_word[word]/max_freq)\n",
        "freq_word.most_common(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP557r56fCyB"
      },
      "source": [
        "sent_strength={}\n",
        "for sent in doc.sents:\n",
        "    for word in sent:\n",
        "        if word.text in freq_word.keys():\n",
        "            if sent in sent_strength.keys():\n",
        "                sent_strength[sent]+=freq_word[word.text]\n",
        "            else:\n",
        "                sent_strength[sent]=freq_word[word.text]\n",
        "print(sent_strength)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmo4YJGjfC1S"
      },
      "source": [
        "summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)\n",
        "print(summarized_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhHc5SRv_VrG"
      },
      "source": [
        "**Text Summarization - Cosine Similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK1hWLsoWZX1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I33HvzA_ZgF"
      },
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnVJiSJ1eZKt"
      },
      "source": [
        "def read_article(file_name):\n",
        "  file = open(file_name, 'r') \n",
        "  filedata = file.readlines()\n",
        "  filedata = [x for x in filedata if x != '\\n'] \n",
        "  filedata = [x.replace('\\n',' ') for x in filedata] \n",
        "  filedata = ''.join(filedata) \n",
        "  filedata = filedata.split('. ') \n",
        "  sentences = []\n",
        "  for sentence in filedata:\n",
        "    sentences.append(sentence.replace('[^a-zA-Z]', ' ').split(' '))\n",
        " \n",
        "  return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ5ZnupePBY0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yBRd9wvVq8M"
      },
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    \n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    \n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FjiyV2HA1-2"
      },
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    \n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        " \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue \n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7NRAUWvAq1s"
      },
      "source": [
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    \n",
        "    sentences =  read_article(file_name)\n",
        "    \n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG9yTzVePBdu"
      },
      "source": [
        "generate_summary(\"new_test.txt\", 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaZWuMdfs9Z6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzbgZhi6s9wr"
      },
      "source": [
        "## **Pending - Encompassing and Deploying it as Flask Application**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlYPjwkOvIMe"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFSf16qfPBgg"
      },
      "source": [
        "from flask import Flask, Response\n",
        "import requests\n",
        "from requests import *\n",
        "import base64\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from summarizer import Summarizer\n",
        "from flask import Flask, jsonify, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "@app.route('/webhook')\n",
        "def Trigger():\n",
        "    text_data = 'Text mining is used to help us specific research questions.  You want to answer why cats sit on mats?  It would be impossible for you to read all the millions of research articles on the topic.  Your south here is where text money can help. It filters large amounts of research and extracts the relevant information, For example, In this instance, the program would identify. The cat is the inner city suburb and armies. The preposition.  It is not just a search tool. It can also understand that the cat is an animal it is in action and the man is an object. It then identifies and maps patents in trend across the millions of articles, for example, it can even tell us if most of the cats who sit on this detailed relevant information helps us determine what additional research is needed in order to answer our questions. So now we go back into the lab with a head start in order to do further research to find out the exact reason, although it might seem easy, text mining, the lot of different tools and resources to make this work find out more by viewing TDM video.'\n",
        "    def Bert_Sum(input_data):\n",
        "        model = Summarizer()\n",
        "        result = model(input_data, min_length=40, max_length=512)\n",
        "        summary_bert = \"\".join(result)\n",
        "        # print(\"The Summary of BERT Model is as follows:  \\n\")\n",
        "        print(summary_bert)\n",
        "        return (summary_bert)\n",
        "\n",
        "    Bert_Sum(text_data)\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def hello():\n",
        "    text_data = 'Text mining is used to help us specific research questions.  You want to answer why cats sit on mats?  It would be impossible for you to read all the millions of research articles on the topic.  Your south here is where text money can help. It filters large amounts of research and extracts the relevant information, For example, In this instance, the program would identify. The cat is the inner city suburb and armies. The preposition.  It is not just a search tool. It can also understand that the cat is an animal it is in action and the man is an object. It then identifies and maps patents in trend across the millions of articles, for example, it can even tell us if most of the cats who sit on this detailed relevant information helps us determine what additional research is needed in order to answer our questions. So now we go back into the lab with a head start in order to do further research to find out the exact reason, although it might seem easy, text mining, the lot of different tools and resources to make this work find out more by viewing TDM video.'\n",
        "    def Bert_Sum(input_data):\n",
        "        model = Summarizer()\n",
        "        result = model(input_data, min_length=40, max_length=512)\n",
        "        summary_bert = \"\".join(result)\n",
        "        # print(\"The Summary of BERT Model is as follows:  \\n\")\n",
        "        print(summary_bert)\n",
        "        \n",
        "\n",
        "    Bert_Sum(text_data)\n",
        "    return (summary_bert)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  app.run()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3_MekqaPBjt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LP65vdyN4I4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV-Adr41N4Mz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}